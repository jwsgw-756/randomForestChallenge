---
title: "Random Forest Analysis"
subtitle: "The Power of Ensemble Learning"
format:
  html: default
execute:
  echo: false
  eval: true
---



```{r}
#| label: load-and-model-r
#| echo: false
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  # Convert zipCode to factor (categorical variable) - important for proper modeling
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()


# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1, mtry = 3, seed = 123)
rf_5 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5, mtry = 3, seed = 123)
rf_25 <- randomForest(SalePrice ~ ., data = train_data, ntree = 25, mtry = 3, seed = 123)
rf_100 <- randomForest(SalePrice ~ ., data = train_data, ntree = 100, mtry = 3, seed = 123)
rf_500 <- randomForest(SalePrice ~ ., data = train_data, ntree = 500, mtry = 3, seed = 123)
rf_1000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1000, mtry = 3, seed = 123)
rf_2000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 2000, mtry = 3, seed = 123)
rf_5000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5000, mtry = 3, seed = 123)
```

## Performance Analysis

```{r}
#| label: performance-comparison-r
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions and performance metrics for test data
predictions_1_test <- predict(rf_1, test_data)
predictions_5_test <- predict(rf_5, test_data)
predictions_25_test <- predict(rf_25, test_data)
predictions_100_test <- predict(rf_100, test_data)
predictions_500_test <- predict(rf_500, test_data)
predictions_1000_test <- predict(rf_1000, test_data)
predictions_2000_test <- predict(rf_2000, test_data)
predictions_5000_test <- predict(rf_5000, test_data)

# Calculate predictions for training data
predictions_1_train <- predict(rf_1, train_data)
predictions_5_train <- predict(rf_5, train_data)
predictions_25_train <- predict(rf_25, train_data)
predictions_100_train <- predict(rf_100, train_data)
predictions_500_train <- predict(rf_500, train_data)
predictions_1000_train <- predict(rf_1000, train_data)
predictions_2000_train <- predict(rf_2000, train_data)
predictions_5000_train <- predict(rf_5000, train_data)

# Calculate RMSE for test data
rmse_1_test <- sqrt(mean((test_data$SalePrice - predictions_1_test)^2))
rmse_5_test <- sqrt(mean((test_data$SalePrice - predictions_5_test)^2))
rmse_25_test <- sqrt(mean((test_data$SalePrice - predictions_25_test)^2))
rmse_100_test <- sqrt(mean((test_data$SalePrice - predictions_100_test)^2))
rmse_500_test <- sqrt(mean((test_data$SalePrice - predictions_500_test)^2))
rmse_1000_test <- sqrt(mean((test_data$SalePrice - predictions_1000_test)^2))
rmse_2000_test <- sqrt(mean((test_data$SalePrice - predictions_2000_test)^2))
rmse_5000_test <- sqrt(mean((test_data$SalePrice - predictions_5000_test)^2))

# Calculate RMSE for training data
rmse_1_train <- sqrt(mean((train_data$SalePrice - predictions_1_train)^2))
rmse_5_train <- sqrt(mean((train_data$SalePrice - predictions_5_train)^2))
rmse_25_train <- sqrt(mean((train_data$SalePrice - predictions_25_train)^2))
rmse_100_train <- sqrt(mean((train_data$SalePrice - predictions_100_train)^2))
rmse_500_train <- sqrt(mean((train_data$SalePrice - predictions_500_train)^2))
rmse_1000_train <- sqrt(mean((train_data$SalePrice - predictions_1000_train)^2))
rmse_2000_train <- sqrt(mean((train_data$SalePrice - predictions_2000_train)^2))
rmse_5000_train <- sqrt(mean((train_data$SalePrice - predictions_5000_train)^2))

# Calculate R-squared
r2_1 <- 1 - sum((test_data$SalePrice - predictions_1_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5 <- 1 - sum((test_data$SalePrice - predictions_5_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_25 <- 1 - sum((test_data$SalePrice - predictions_25_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_100 <- 1 - sum((test_data$SalePrice - predictions_100_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_500 <- 1 - sum((test_data$SalePrice - predictions_500_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_1000 <- 1 - sum((test_data$SalePrice - predictions_1000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_2000 <- 1 - sum((test_data$SalePrice - predictions_2000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5000 <- 1 - sum((test_data$SalePrice - predictions_5000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create performance comparison
performance_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

print(performance_df)
```

## 1. The Power of More Trees Visualization

```{r}
#| label: visualization-power-of-trees
#| echo: false
#| message: false
#| warning: false
#| fig-width: 12
#| fig-height: 5

# Create the visualization showing RMSE and R-squared vs Number of Trees
library(ggplot2)
library(gridExtra)

# RMSE Plot
rmse_plot <- ggplot(performance_df, aes(x = Trees)) +
  geom_line(aes(y = RMSE_Test, color = "Test Data"), size = 1.2) +
  geom_line(aes(y = RMSE_Train, color = "Training Data"), size = 1.2) +
  geom_point(aes(y = RMSE_Test, color = "Test Data"), size = 2) +
  geom_point(aes(y = RMSE_Train, color = "Training Data"), size = 2) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_color_manual(values = c("Test Data" = "#E31A1C", "Training Data" = "#1F78B4")) +
  labs(
    title = "RMSE vs Number of Trees",
    x = "Number of Trees (log scale)",
    y = "RMSE",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )

# R-squared Plot
r2_plot <- ggplot(performance_df, aes(x = Trees, y = R_squared)) +
  geom_line(color = "#2E8B57", size = 1.2) +
  geom_point(color = "#2E8B57", size = 2) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  labs(
    title = "R-squared vs Number of Trees",
    x = "Number of Trees (log scale)",
    y = "R-squared"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    panel.grid.minor = element_blank()
  )

# Combine plots
grid.arrange(rmse_plot, r2_plot, ncol = 2)
```

### Analysis: The Power of Ensemble Learning

The visualization reveals several key insights about the relationship between the number of trees and model performance:

**Area of Largest Improvement:** The most significant performance gains occur in the early stages of ensemble building, particularly from 1 to 25 trees. The RMSE drops dramatically from approximately 45,000 to 35,000 (a 22% improvement) and the R-squared jumps from 0.65 to 0.80 (a 23% improvement). This range represents the area of largest improvement with roughly 20% improvements across the board, every additional tree from here seems to negatively impact performance.

**Diminishing Returns:** Beyond 100 trees, the performance improvements become increasingly marginal. While adding trees from 100 to 5000 still provides marginally noticeable benefits (RMSE continues to decrease and R-squared continues to slightly increase), the rate of improvement slows considerably. The gap between training and test performance remains relatively stable, indicating that the model is not overfitting despite the large number of trees.

**Practical Implications:** For this dataset, the "sweet spot" appears to be around 100-500 trees, where we achieve most of the performance benefits without excessive computational cost with performance loss. The ensemble effect is most pronounced in the early stages, demonstrating why random forests are so effective - even a modest number of weak learners can create a surprisingly strong predictor.


## 2. Overfitting Analysis

```{r}
#| label: overfitting-analysis
#| echo: false
#| message: false
#| warning: false
#| fig-width: 14
#| fig-height: 6

# Load required libraries
library(rpart)
library(ggplot2)
library(gridExtra)

# Create decision trees with different max depths
set.seed(123)
dt_1 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(maxdepth = 1))
dt_2 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(maxdepth = 2))
dt_3 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(maxdepth = 3))
dt_5 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(maxdepth = 5))
dt_10 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(maxdepth = 10))
dt_15 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(maxdepth = 15))
dt_20 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(maxdepth = 20))

# Calculate predictions and RMSE for decision trees
dt_depths <- c(1, 2, 3, 5, 10, 15, 20)
dt_models <- list(dt_1, dt_2, dt_3, dt_5, dt_10, dt_15, dt_20)

dt_rmse_train <- sapply(dt_models, function(model) {
  pred <- predict(model, train_data)
  sqrt(mean((train_data$SalePrice - pred)^2))
})

dt_rmse_test <- sapply(dt_models, function(model) {
  pred <- predict(model, test_data)
  sqrt(mean((test_data$SalePrice - pred)^2))
})

# Create data frames for plotting
dt_performance <- data.frame(
  MaxDepth = dt_depths,
  RMSE_Train = dt_rmse_train,
  RMSE_Test = dt_rmse_test
)

# Random forest performance (using existing data)
rf_performance <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test)
)

# Calculate common y-axis limits for both plots
all_rmse_values <- c(dt_performance$RMSE_Train, dt_performance$RMSE_Test, 
                    rf_performance$RMSE_Train, rf_performance$RMSE_Test)
y_min <- min(all_rmse_values) * 0.95
y_max <- max(all_rmse_values) * 1.05

# Create decision tree plot
dt_plot <- ggplot(dt_performance, aes(x = MaxDepth)) +
  geom_line(aes(y = RMSE_Test, color = "Test Data"), size = 1.2) +
  geom_line(aes(y = RMSE_Train, color = "Training Data"), size = 1.2) +
  geom_point(aes(y = RMSE_Test, color = "Test Data"), size = 2) +
  geom_point(aes(y = RMSE_Train, color = "Training Data"), size = 2) +
  scale_color_manual(values = c("Test Data" = "#E31A1C", "Training Data" = "#1F78B4")) +
  scale_y_continuous(limits = c(y_min, y_max)) +
  labs(
    title = "Decision Trees: Overfitting with Complexity",
    x = "Maximum Depth",
    y = "RMSE",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )

# Create random forest plot
rf_plot <- ggplot(rf_performance, aes(x = Trees)) +
  geom_line(aes(y = RMSE_Test, color = "Test Data"), size = 1.2) +
  geom_line(aes(y = RMSE_Train, color = "Training Data"), size = 1.2) +
  geom_point(aes(y = RMSE_Test, color = "Test Data"), size = 2) +
  geom_point(aes(y = RMSE_Train, color = "Training Data"), size = 2) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_y_continuous(limits = c(y_min, y_max)) +
  scale_color_manual(values = c("Test Data" = "#E31A1C", "Training Data" = "#1F78B4")) +
  labs(
    title = "Random Forests: No Overfitting with More Trees",
    x = "Number of Trees (log scale)",
    y = "RMSE",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )

# Combine plots side by side
grid.arrange(dt_plot, rf_plot, ncol = 2)
```

### Analysis: Overfitting in Decision Trees vs Random Forests

The side-by-side comparison reveals fundamental differences in how decision trees and random forests handle complexity:

**Why Decision Trees Overfit:**
As decision trees become more complex, they create increasingly specific rules that perfectly fit the training data. This leads to a growing gap between training and test performance - the classic sign of overfitting. The model becomes somewhat useless for new data and is only really really good at model and predicting the training data with accuracy.

**Why Random Forests Don't Overfit:**
Random forests maintain stable performance across increasing numbers of trees because of three key mechanisms:

1. **Bootstrap Sampling:** Each tree trains on a different random subset of data, preventing any single tree from memorizing the entire training set and losing true predictive power to better model training data.
2. **Random Feature Selection:** Each split considers only a random subset of features, reducing the chance of finding spurious patterns
3. **Averaging:** The final prediction averages across many diverse trees, canceling out individual overfitting tendencies and reflects the "wisdom of the crowd"

**The Key Insight:** While individual trees in a random forest may overfit, their diversity and the averaging process create a robust ensemble that generalizes well. This is why random forests can use many trees without suffering from overfitting - the ensemble effect transforms weak, potentially overfitted learners into a strong, generalizable model by using the entire forest of trees.


## 3. Linear Regression Comparison

```{r}
#| label: linear-regression-comparison
#| echo: false
#| message: false
#| warning: false

# Create linear regression model
set.seed(123)
lm_model <- lm(SalePrice ~ ., data = train_data)

# Calculate linear regression predictions and RMSE
lm_pred_train <- predict(lm_model, train_data)
lm_pred_test <- predict(lm_model, test_data)
lm_rmse_train <- sqrt(mean((train_data$SalePrice - lm_pred_train)^2))
lm_rmse_test <- sqrt(mean((test_data$SalePrice - lm_pred_test)^2))

# Calculate R-squared for linear regression
lm_r2 <- 1 - sum((test_data$SalePrice - lm_pred_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create comparison table
comparison_table <- data.frame(
  Model = c("Linear Regression", "Random Forest (1 tree)", "Random Forest (100 trees)", "Random Forest (1000 trees)"),
  RMSE_Test = c(lm_rmse_test, rmse_1_test, rmse_100_test, rmse_1000_test),
  RMSE_Train = c(lm_rmse_train, rmse_1_train, rmse_100_train, rmse_1000_train),
  R_squared = c(lm_r2, r2_1, r2_100, r2_1000)
)

# Calculate percentage improvements over linear regression
comparison_table$Improvement_Test <- round((lm_rmse_test - comparison_table$RMSE_Test) / lm_rmse_test * 100, 1)
comparison_table$Improvement_R2 <- round((comparison_table$R_squared - lm_r2) / lm_r2 * 100, 1)

# Create a visually appealing table using kableExtra
library(knitr)
library(kableExtra)

# Format the table for better presentation
formatted_table <- comparison_table %>%
  mutate(
    RMSE_Test = paste0("$", formatC(RMSE_Test, format = "f", big.mark = ",", digits = 0)),
    RMSE_Train = paste0("$", formatC(RMSE_Train, format = "f", big.mark = ",", digits = 0)),
    R_squared = paste0(formatC(R_squared, format = "f", digits = 3)),
    Improvement_Test = paste0("+", Improvement_Test, "%"),
    Improvement_R2 = paste0("+", Improvement_R2, "%")
  ) %>%
  select(Model, RMSE_Test, RMSE_Train, R_squared, Improvement_Test, Improvement_R2)

# Create the styled table
kable(formatted_table, 
      caption = "Model Performance Comparison: Linear Regression vs Random Forests",
      col.names = c("Model", "Test RMSE", "Train RMSE", "R²", "RMSE Improvement", "R² Improvement"),
      align = c("l", "r", "r", "r", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE,
                position = "center") %>%
  row_spec(1, background = "#f0f8ff", bold = TRUE) %>%  # Linear regression
  row_spec(2, background = "#fff0f0") %>%  # 1 tree
  row_spec(3, background = "#f0fff0") %>%  # 100 trees
  row_spec(4, background = "#f5f5f5") %>%  # 1000 trees
  column_spec(1, bold = TRUE) %>%
  add_header_above(c(" " = 1, "Performance Metrics" = 3, "Improvement over Linear Regression" = 2))

# Calculate and display key insights
rf_1_to_100_improvement <- round((rmse_1_test - rmse_100_test) / rmse_1_test * 100, 1)
rf_100_vs_lm_improvement <- round((lm_rmse_test - rmse_100_test) / lm_rmse_test * 100, 1)

```

### Analysis: Linear Regression vs Random Forest Trade-offs

The comparison reveals important insights about when to choose random forests over linear regression:

**Performance Improvements:**

- 1 tree to 100 trees: The improvement is substantial, showing the power of ensemble learning
- Linear regression to 100-tree random forest: The improvement demonstrates that random forests can capture non-linear relationships that linear regression misses. The table doesn't necessarily show this because the dataset was all properties from the Ames dataset which doesn't vary enough to show the difference.

**When Random Forests Are Worth the Complexity:**

Random forests are particularly valuable when:

- Non-linear relationships exist between features and target (as in real estate pricing)
- Features interact in a non-linear manner, a square foot difference affects price differently in different zip codes
- Performance is critical and computational cost is acceptable
- When you are willing to sacrifice a bit of interpretability for more powerful performance for potentially non-linear relationships